{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow-probability==0.15.0 \n",
    "!pip install -U tensorflow-io\n",
    "!pip install -U tensorflow_model_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:39.743506Z",
     "iopub.status.busy": "2022-01-26T02:32:39.742922Z",
     "iopub.status.idle": "2022-01-26T02:32:41.587375Z",
     "shell.execute_reply": "2022-01-26T02:32:41.587817Z"
    },
    "id": "WZKbyU2-AiY-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import signal, stats\n",
    "import librosa\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "\n",
    "from pystoi import stoi\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from scipy.io.wavfile import write\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, UpSampling2D, Flatten, Dropout, Dense, Reshape, \\\n",
    "    Conv2DTranspose, BatchNormalization, Activation, MaxPooling2D, Lambda, Concatenate, Multiply, Add\n",
    "from tensorflow.keras import Model, Sequential, initializers # Data Generator\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:41.608205Z",
     "iopub.status.busy": "2022-01-26T02:32:41.603269Z",
     "iopub.status.idle": "2022-01-26T02:32:49.530847Z",
     "shell.execute_reply": "2022-01-26T02:32:49.531240Z"
    },
    "id": "YzTlj4YdCip_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 256\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(0)\n",
    "def normalize(matrix):\n",
    "    data = (matrix - tf.experimental.numpy.min(matrix)) / (tf.experimental.numpy.max(matrix) - tf.experimental.numpy.min(matrix))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CH_1LFwtJ6b"
   },
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_dB = 5\n",
    "sampling_rate = 4000\n",
    "\n",
    "def awgn(sinal, noise_signal):\n",
    "    regsnr=snr_dB\n",
    "    sigpower=sum([math.pow(abs(sinal[i]),2) for i in range(len(sinal))])\n",
    "    sigpower=sigpower/len(sinal)\n",
    "    sig_dB = 10* math.log(sigpower, 10)\n",
    "    noise_dB = sig_dB-snr_dB\n",
    "    noisescale= (math.pow(10,noise_dB/20))\n",
    "    npower=sum([math.pow(abs(noise_signal[i]),2) for i in range(len(noise_signal))])\n",
    "    npower=math.sqrt(npower/len(noise_signal))\n",
    "    noise=noisescale*noise_signal/npower#math.sqrt(noisepower)*noise_signal\n",
    "    return noise\n",
    "\n",
    "def get_stft(x, fs, n_fft, hop_length, only_real=True):\n",
    "    c_stft = librosa.stft(x, n_fft=n_fft, hop_length=hop_length)\n",
    "    if only_real:\n",
    "        return np.abs(c_stft)\n",
    "    else:\n",
    "        return c_stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer_id = 1\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "clean_dir = \"../Dataset/Interfered/Volunteer\"+str(volunteer_id)\n",
    "\n",
    "checkpoint_dir = './volunteer_'+str(volunteer_id)+'_training_checkpoints'\n",
    "cycle_checkpoint_dir = './volunteer_'+str(volunteer_id)+'_cycle_training_checkpoints'\n",
    "\n",
    "output_parent_folder_name = \"volunteer_\"+str(volunteer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the files. \n",
    "with (open(os.path.join(clean_dir, \"target_train.p\"), \"rb\")) as target_train_file:\n",
    "    all_train_mic_time, _, all_train_mic_stft, all_train_imu_stft, all_train_labels = pickle.load(target_train_file)\n",
    "with (open(os.path.join(clean_dir, \"target_test.p\"), \"rb\")) as target_test_file:\n",
    "    all_test_mic_time, _, all_test_mic_stft, all_test_imu_stft, all_test_labels = pickle.load(target_test_file)\n",
    "with (open(os.path.join(clean_dir, \"noise_train.p\"), \"rb\")) as noise_train_file:\n",
    "    all_train_noise_time, _, all_train_noise_labels = pickle.load(noise_train_file)\n",
    "with (open(os.path.join(clean_dir, \"noise_test.p\"), \"rb\")) as noise_test_file:\n",
    "    all_test_noise_time, _, all_test_noise_labels = pickle.load(noise_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_train_mic_time.shape, all_train_mic_stft.shape, all_train_imu_stft.shape)\n",
    "print(all_test_mic_time.shape, all_test_mic_stft.shape, all_test_imu_stft.shape)\n",
    "print(all_train_noise_time.shape)\n",
    "print(all_test_noise_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read combination files\n",
    "with (open(os.path.join(clean_dir, \"combination_index_train.p\"), \"rb\")) as index_train_file:\n",
    "    train_indexes = pickle.load(index_train_file)\n",
    "with (open(os.path.join(clean_dir, \"combination_index_test.p\"), \"rb\")) as index_test_file:\n",
    "    test_indexes = pickle.load(index_test_file)\n",
    "    \n",
    "print(len(train_indexes), len(test_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_gen(batch_size, ftype = \"train\", real_output = True, onlyAudio = False, batch_ind = 0):\n",
    "    if ftype==\"train\":\n",
    "        all_mic_time = all_train_mic_time\n",
    "        all_mic_stft = all_train_mic_stft\n",
    "        all_imu_stft = all_train_imu_stft\n",
    "        all_noise_time = all_train_noise_time\n",
    "        all_target_labels = all_train_labels\n",
    "        all_noise_labels = all_train_noise_labels\n",
    "        all_indexes = train_indexes\n",
    "    else:\n",
    "        all_mic_time = all_test_mic_time\n",
    "        all_mic_stft = all_test_mic_stft\n",
    "        all_imu_stft = all_test_imu_stft\n",
    "        all_noise_time = all_test_noise_time\n",
    "        all_target_labels = all_test_labels\n",
    "        all_noise_labels = all_test_noise_labels\n",
    "        all_indexes = test_indexes\n",
    "\n",
    "    indxs = all_indexes[batch_size*batch_ind: batch_size*batch_ind + batch_size]\n",
    "    indxs = np.asarray(indxs)\n",
    "\n",
    "    noisy_arr_stft = []\n",
    "    target_arr_mic_stft = []\n",
    "    target_arr_imu_stft = []\n",
    "    label_arr = []\n",
    "    noise_arr = []\n",
    "    target_mic_arr = []\n",
    "    noisy_mic_arr = []\n",
    "\n",
    "    for ind in indxs:\n",
    "        temp_noise_time = all_noise_time[ind[1][0]][ind[1][1]]\n",
    "        temp_noise_labels = all_noise_labels[ind[1][0]][ind[1][1]]\n",
    "        temp_mic_time = all_mic_time[ind[0][0]][ind[0][1]][ind[0][2]]\n",
    "        temp_mic_stft = all_mic_stft[ind[0][0]][ind[0][1]][ind[0][2]]\n",
    "        temp_imu_stft = all_imu_stft[ind[0][0]][ind[0][1]][ind[0][2]]\n",
    "        temp_target_labels = all_target_labels[ind[0][0]][ind[0][1]][ind[0][2]]\n",
    "\n",
    "        scaled_noise = awgn(temp_mic_time, temp_noise_time)\n",
    "        roll_idx = random.randint(scaled_noise.shape[0]//2, scaled_noise.shape[0])\n",
    "        scaled_noise_r = np.roll(scaled_noise, roll_idx)\n",
    "        noisy_time = np.add(temp_mic_time, scaled_noise_r)\n",
    "        \n",
    "        temp = get_stft(noisy_time, fs=noisy_time.shape[0], n_fft=400, hop_length=200, only_real=False)\n",
    "        temp = temp[1:201, :]\n",
    "        \n",
    "        temp_mic_stft = temp_mic_stft[1:201, :]\n",
    "        temp_imu_stft = temp_imu_stft[1:21, :]\n",
    "        \n",
    "        noisy_mic_arr.append(noisy_time)\n",
    "        noisy_arr_stft.append(temp)\n",
    "        target_mic_arr.append(temp_mic_time)\n",
    "        noise_arr.append(scaled_noise_r)\n",
    "        target_arr_mic_stft.append(temp_mic_stft)\n",
    "        target_arr_imu_stft.append(temp_imu_stft)\n",
    "        label_arr.append([temp_target_labels, temp_noise_labels])\n",
    "\n",
    "    noisy_arr_stft = np.asarray(noisy_arr_stft)\n",
    "    target_arr_mic_stft = np.asarray(target_arr_mic_stft)\n",
    "    target_arr_imu_stft = np.asarray(target_arr_imu_stft)\n",
    "    label_arr = np.asarray(label_arr)\n",
    "    target_mic_arr = np.asarray(target_mic_arr)\n",
    "    noise_arr = np.asarray(noise_arr)\n",
    "    noisy_mic_arr = np.asarray(noisy_mic_arr)\n",
    "\n",
    "\n",
    "    noisy_arr_stft = noisy_arr_stft.reshape(noisy_arr_stft.shape[0], noisy_arr_stft.shape[1], noisy_arr_stft.shape[2], 1)\n",
    "\n",
    "    target_arr_mic_stft = target_arr_mic_stft.reshape(target_arr_mic_stft.shape[0], target_arr_mic_stft.shape[1], target_arr_mic_stft.shape[2], 1)\n",
    "    target_arr_imu_stft = target_arr_imu_stft.reshape(target_arr_imu_stft.shape[0], target_arr_imu_stft.shape[1], target_arr_imu_stft.shape[2], 1)\n",
    "\n",
    "    output = np.abs(target_arr_mic_stft)\n",
    "\n",
    "\n",
    "    return noisy_arr_stft, np.abs(target_arr_imu_stft), output, label_arr, target_mic_arr, noise_arr, noisy_mic_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "noisy_audio, input_imu, clean_audio, labels, target_mic_arr, noise_arr, noisy_mic_arr = dataset_gen(batch_size=test_batch_size, ftype=\"test\")\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label dictionary for KWS10\n",
    "label_dict = {'bird': 1,\n",
    " 'happy': 1,\n",
    " 'cat': 1,\n",
    " 'dog': 1,\n",
    " 'follow': 1,\n",
    " 'house': 1,\n",
    " 'forward': 1,\n",
    " 'bed': 1,\n",
    " 'backward': 1,\n",
    " 'sheila': 1,\n",
    " 'tree': 1,\n",
    " 'two': 1,\n",
    " 'down': 5,\n",
    " 'four': 1,\n",
    " 'eight': 1,\n",
    " 'visual': 1,\n",
    " 'five': 1,\n",
    " 'marvin': 1,\n",
    " 'go': 11,\n",
    " 'learn': 1,\n",
    " 'wow': 1,\n",
    " 'left': 6,\n",
    " 'one': 1,\n",
    " 'seven': 1,\n",
    " 'off': 9,\n",
    " 'nine': 1,\n",
    " 'right': 7,\n",
    " 'up': 4,\n",
    " 'stop': 10,\n",
    " 'zero': 1,\n",
    " 'three': 1,\n",
    " 'on': 8,\n",
    " 'yes': 2,\n",
    " 'six': 1,\n",
    " 'no': 3,\n",
    " '_silence_': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_dict[labels[0][0]],  labels[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Create the models\n",
    "\n",
    "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_denoiser(l2_strength):\n",
    "    \n",
    "    \n",
    "    input_imu = Input(shape=[20,21, 1])\n",
    "    input_audio = Input(shape=[200,21,1])\n",
    "    input_filter_bob = Input(shape=[20,21,1])\n",
    "    x = input_audio\n",
    "\n",
    "    # ----- 2\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 1),padding='same')(x)\n",
    "\n",
    "    skip0 = Conv2D(filters=64, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    \n",
    "\n",
    "    x = Activation('relu')(skip0)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # ----- 3\n",
    "    x = Conv2D(filters=16, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 1),padding='same')(x)\n",
    "\n",
    "    skip1 = Conv2D(filters=8, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(skip1)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=4, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # ---- 4 Only shrink on IMU space\n",
    "    x = Conv2D(filters=2, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Cut IMU part out\n",
    "    latent = x # Should be 50x21xc, first 20x21xc is IMU latent space\n",
    "    imu_ = Lambda(lambda x: x[:,0:20,:,:])(latent)\n",
    "    \n",
    "    #rest =  Lambda(lambda x: x[:,20:,:,:])(latent)     \n",
    "    alice =  Lambda(lambda x: x[:,20:30,:,:])(latent) \n",
    "    bob = Lambda(lambda x: x[:,30:,:,:])(latent) \n",
    "    bob = Lambda(lambda x: x[0]*x[1])((bob,input_filter_bob))\n",
    "    rest = Concatenate(axis=1)([alice, bob])\n",
    "    # latent space operations    \n",
    "    \n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=2, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(rest)\n",
    "    x = Activation('relu')(x) # 30x21x2\n",
    "    \n",
    "    \n",
    "    #imu = Lambda(lambda x: x[2]*x[0]+(1-x[2])*x[1])((imu_,input_imu,input_filter_imu))\n",
    "    imu = Concatenate(axis=3)([imu_, input_imu]) # 20x21x2\n",
    "    x = Concatenate(axis=1)([imu, x]) \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    \n",
    "    # ---- 5\n",
    "    x = Conv2D(filters=4, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=8, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = x + 0.1*skip1\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # Add back rest\n",
    "    \n",
    "    \n",
    "    x = Conv2DTranspose(filters=16, kernel_size=[7,1], strides=[2, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # ---- 6\n",
    "    x = Conv2D(filters=32, kernel_size=[7,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "             kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = x #+ skip0\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=32, kernel_size=[7,1], strides=[2, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # ---- 7\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n",
    "    x = Conv2D(filters=1, kernel_size=[21,1], strides=[1, 1], padding='same')(x)\n",
    "\n",
    "    #denoiser = Model(inputs=[input_audio, input_imu], outputs=x)    \n",
    "    denoiser = Model(inputs=[input_audio, input_imu, input_filter_bob], outputs=[x,latent])    \n",
    "    return denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = make_denoiser(l2_strength=0.002)\n",
    "denoiser.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_audio = denoiser([np.abs(noisy_audio),input_imu,input_imu])\n",
    "temp = denoised_audio[0]\n",
    "print(denoised_audio[0][0,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_translator():\n",
    "    input_imu = Input(shape=[20,21, 1])\n",
    "    \n",
    "    # Encoder\n",
    "    x = Conv2D(filters=128, kernel_size=3,padding='same', name=\"enc_conv1\")(input_imu)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', name=\"enc_conv2\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((5,1), name=\"enc_maxpool1\")(x)\n",
    "    \n",
    "    x = Conv2D(filters=16, kernel_size=3, strides=1, padding='same', name=\"enc_conv3\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2D(filters=16, kernel_size=3, strides=1, padding='same', name=\"dec_conv1\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = UpSampling2D((2,1), name=\"dec_up1\")(x)\n",
    "#     up1 = Conv2D(filters=1, kernel_size=3,  padding='same', name=\"up1\")(x)\n",
    "    \n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same', name=\"dec_conv2\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Conv2D(filters=1, kernel_size=3,  padding='same', name=\"dec_conv3\")(x)\n",
    "    x = UpSampling2D((5,1), name=\"dec_up2\")(x)\n",
    "    up2 = Conv2D(filters=1, kernel_size=3,  padding='same', name=\"up2\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', name=\"dec_conv4\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = UpSampling2D((5,1), name=\"dec_up3\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=3, strides=1, padding='same', name=\"dec_conv5\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=1, kernel_size=3, padding='same', name=\"dec_conv6\")(x)\n",
    "\n",
    "    translator = Model(inputs=input_imu, outputs=[up2, x])  #outputs=[up1, up2, x])    \n",
    "    \n",
    "    return translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = make_translator()\n",
    "translator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_audio = translator(input_imu)\n",
    "\n",
    "print(translated_audio[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(targets, outputs):\n",
    "    mse = tf.keras.losses.MeanAbsoluteError()\n",
    "    return mse(targets, outputs)\n",
    "def mse2(targets, outputs):\n",
    "    mse2 = tf.keras.losses.MeanSquaredError()\n",
    "    return mse2(targets, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft2time(target_stft, noisy_audio):\n",
    "    theta_n = np.angle(noisy_audio.reshape(noisy_audio.shape[0], noisy_audio.shape[1]))\n",
    "    x = target_stft.reshape([target_stft.shape[0], target_stft.shape[1]])\n",
    "    complex_noisy = x * np.exp(1j*theta_n)\n",
    "    \n",
    "    temp_z = np.zeros([1,21])\n",
    "    temp = np.hstack((temp_z.T, complex_noisy.T)).T\n",
    "    reconstructed_noisy_mic = librosa.istft(temp, hop_length=200, length=4000)\n",
    "    return reconstructed_noisy_mic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator_loss(translator_output_arr, denoiser_output_arr, noisy_audio_arr, cycle):\n",
    "\n",
    "    final_out = []\n",
    "    down1_out = []\n",
    "    for i in range(0, translator_output_arr[0].shape[0]):\n",
    "        noisy_audio = noisy_audio_arr[i]\n",
    "        denoiser_output = denoiser_output_arr[i]\n",
    "        reconstructed_noisy_mic = stft2time(denoiser_output, noisy_audio)\n",
    "    \n",
    "        down_noisy_time1 = signal.resample(reconstructed_noisy_mic, 800)\n",
    "        down1 = get_stft(down_noisy_time1, fs=down_noisy_time1.shape[0], n_fft=80, hop_length=40, only_real=True)\n",
    "        down1 = down1[1:41, :]\n",
    "        down1 = down1.reshape([down1.shape[0], down1.shape[1], 1])\n",
    "        down1_out.append(down1)\n",
    "        \n",
    "        final_out.append(denoiser_output)\n",
    "    final_out = np.asarray(final_out, dtype=np.float32)\n",
    "    down1_out = np.asarray(down1_out, dtype=np.float32)\n",
    "\n",
    "    t1_loss = mse(translator_output_arr[0][i], down1_out)\n",
    "    t_loss = mse(translator_output_arr[1][i], final_out)\n",
    "    t_loss += t1_loss\n",
    "    \n",
    "    return t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(translator_output, noisy_or_denoiser_output, percentile1, percentile2):\n",
    "    d_prev_out = noisy_or_denoiser_output.reshape([200, 21])\n",
    "    t_out = translator_output.reshape([200,21])\n",
    "    \n",
    "    d_prev_db = librosa.amplitude_to_db(d_prev_out,ref=80)\n",
    "    t_db =librosa.amplitude_to_db(t_out,ref=80)\n",
    "\n",
    "    summing = np.percentile(d_prev_db, percentile1, axis = 0)\n",
    "    \n",
    "    ref = (np.max(summing)+np.min(summing))/2\n",
    "    ref_id = np.where(summing>ref)\n",
    "\n",
    "    arr = np.arange(0, 21)\n",
    "    c=np.abs([arr-x for x in ref_id[0]])\n",
    "    c = np.min(c,axis=0)\n",
    "\n",
    "    idx = np.where(c>margin)\n",
    "\n",
    "    truncated_t = t_db.reshape([200,21])[:, idx]\n",
    "    \n",
    "    for j in range(0, 4):\n",
    "        target_data = truncated_t[j*50: j*50+50]\n",
    "        thr = np.percentile(target_data, percentile2)\n",
    "        t_sig = tf.math.sigmoid(t_db[j*50: j*50+50]-thr)\n",
    "        if j == 0:\n",
    "            stiched_data = t_sig\n",
    "        else:\n",
    "            stiched_data = np.concatenate([stiched_data, t_sig], axis=0)\n",
    "\n",
    "\n",
    "    mask = noisy_or_denoiser_output.reshape([200,21]) * stiched_data\n",
    "    \n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 0\n",
    "percentile1 = 80\n",
    "percentile2 = 90\n",
    "theta = 1e-5\n",
    "alpha_1 = 0.05\n",
    "beta_1 = 600\n",
    "\n",
    "TRAIN_B_WEIGHT = 0\n",
    "REPLACE_B_WEIGHT = 0\n",
    "\n",
    "pre_weight = tf.fill(input_imu.shape,TRAIN_B_WEIGHT)\n",
    "post_weight = tf.fill(input_imu.shape,REPLACE_B_WEIGHT)\n",
    "\n",
    "ALC_DIM = 10\n",
    "#BOB_DIM = 20\n",
    "def denoiser_loss(translator_output, denoiser_output, denoised_static, noisy_data, input_imu, epoch, batch_size, cycle): \n",
    "    global percentile1, percentile2\n",
    "    \n",
    "    percentile_temp = percentile2 + cycle\n",
    "    d_loss = 0\n",
    "    imu_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        mask = generate_mask(translator_output[1][i].numpy(), denoised_static[i], percentile1, percentile_temp)\n",
    "        mask = mask.reshape([200,21, 1])\n",
    "        temp_loss = mse(denoiser_output[0][i], mask)+alpha_1*mse(denoiser_output[1][i][:20].numpy(), input_imu[i])\n",
    "        \n",
    "        temp2 = mse(denoiser_output[1][i][:20].numpy(), input_imu[i])\n",
    "        d_loss += temp_loss \n",
    "        imu_loss += temp2\n",
    "    \n",
    "    \n",
    "    # Contrastive loss\n",
    "    #imu =  denoiser_output[1][:,:20,:].numpy() # 128*20*21*1\n",
    "    imu = input_imu # 128*20*21*1\n",
    "    imu = np.transpose(imu,(1,0,2,3)).reshape((20,-1)) # 20*...\n",
    "    imu = stats.zscore(imu,axis = 1) # zeros mean, std = 1\n",
    "    rest = denoiser_output[1][:,20:,:].numpy() # 128*30*21*1\n",
    "    rest = np.transpose(rest,(1,0,2,3)).reshape((30,-1)) # 30*...\n",
    "    rest = stats.zscore(rest,axis = 1)\n",
    "    alice_high = rest[:ALC_DIM].T\n",
    "    bob_all = rest[ALC_DIM:].T\n",
    "    a_corr = np.sum(imu@alice_high)/200/128/21\n",
    "    b_corr = np.sum(imu@bob_all)/400/128/21\n",
    "    r_loss = np.abs(b_corr) - np.abs(a_corr)\n",
    "    \n",
    "    d_loss += beta_1*r_loss\n",
    "    \n",
    "    d_loss = d_loss/batch_size\n",
    "    imu_loss = imu_loss/batch_size\n",
    "    \n",
    "    return d_loss,imu_loss,r_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.194322Z",
     "iopub.status.busy": "2022-01-26T02:32:54.193742Z",
     "iopub.status.idle": "2022-01-26T02:32:54.195834Z",
     "shell.execute_reply": "2022-01-26T02:32:54.195275Z"
    },
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "translator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "denoiser_optimizer = tf.keras.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "### Save checkpoints\n",
    "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.200130Z",
     "iopub.status.busy": "2022-01-26T02:32:54.199551Z",
     "iopub.status.idle": "2022-01-26T02:32:54.201569Z",
     "shell.execute_reply": "2022-01-26T02:32:54.201153Z"
    },
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(translator_optimizer=translator_optimizer,\n",
    "                                 denoiser_optimizer=denoiser_optimizer,\n",
    "                                 translator=translator,\n",
    "                                 denoiser=denoiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_checkpoint_prefix = os.path.join(cycle_checkpoint_dir, \"ckpt\")\n",
    "cycle_checkpoint = tf.train.Checkpoint(denoiser_optimizer=denoiser_optimizer,\n",
    "                                 denoiser=denoiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the KWS_10 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kws_streaming.models import model_params\n",
    "from kws_streaming.models import model_flags\n",
    "from kws_streaming.models import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kws_streaming.data.input_data as input_data\n",
    "#import tensorflow.compat.v1 as tf\n",
    "def kws_set_flag():\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    MODEL_NAME = 'ds_tc_resnet'\n",
    "    # MODEL_NAME = 'svdf'\n",
    "    MODELS_PATH = os.path.join(current_dir, \"models\")\n",
    "    MODEL_PATH = os.path.join(MODELS_PATH, MODEL_NAME + \"_40k/\")\n",
    "    MODEL_PATH\n",
    "    FLAGS = model_params.HOTWORD_MODEL_PARAMS[MODEL_NAME]\n",
    "\n",
    "    # set speech feature extractor properties\n",
    "\n",
    "    FLAGS.window_size_ms = 30.0\n",
    "    FLAGS.window_stride_ms = 10.0\n",
    "    FLAGS.mel_num_bins = 80\n",
    "    FLAGS.dct_num_features = 40\n",
    "    FLAGS.feature_type = 'mfcc_tf'\n",
    "    FLAGS.preprocess = 'raw'\n",
    "\n",
    "    # for numerical correctness of streaming and non streaming models set it to 1\n",
    "    # but for real use case streaming set it to 0\n",
    "    FLAGS.causal_data_frame_padding = 0\n",
    "\n",
    "    FLAGS.use_tf_fft = True\n",
    "    FLAGS.mel_non_zero_only = not FLAGS.use_tf_fft\n",
    "\n",
    "    # set training settings\n",
    "    FLAGS.train = 1\n",
    "    # reduced number of training steps for test only\n",
    "    # so model accuracy will be low,\n",
    "    # to improve accuracy set how_many_training_steps = '40000,40000,20000,20000'\n",
    "    FLAGS.how_many_training_steps = '40000,40000,20000,20000'\n",
    "    FLAGS.learning_rate = '0.001,0.0005,0.0001,0.00002'\n",
    "    FLAGS.lr_schedule = 'linear'\n",
    "\n",
    "    # data augmentation parameters\n",
    "    FLAGS.resample = 0.15\n",
    "    FLAGS.time_shift_ms = 100\n",
    "    FLAGS.use_spec_augment = 1\n",
    "    FLAGS.time_masks_number = 2\n",
    "    FLAGS.time_mask_max_size = 25\n",
    "    FLAGS.frequency_masks_number = 2\n",
    "    FLAGS.frequency_mask_max_size = 7\n",
    "    FLAGS.pick_deterministically = 1\n",
    "    \n",
    "    FLAGS.train_dir = MODEL_PATH\n",
    "    FLAGS.sample_rate = 4000\n",
    "    FLAGS.mel_upper_edge_hertz = 2000\n",
    "    FLAGS.model_name = MODEL_NAME\n",
    "    if MODEL_NAME == 'ds_tc_resnet':\n",
    "      # it is an example of model streaming with strided convolution, strided pooling and dilated convolution\n",
    "      FLAGS.activation = 'relu'\n",
    "      FLAGS.dropout = 0.0\n",
    "      FLAGS.ds_filters = '128, 64, 64, 64, 128, 128'\n",
    "      FLAGS.ds_filter_separable = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_repeat = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_residual = '0, 1, 1, 1, 0, 0' # residual can not be applied with stride\n",
    "    #   FLAGS.ds_kernel_size = '11, 5, 15, 7, 29, 1'\n",
    "      FLAGS.ds_kernel_size = '11, 5, 15, 17, 15, 1'\n",
    "      FLAGS.ds_dilation = '1, 1, 1, 1, 2, 1'\n",
    "      FLAGS.ds_stride = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_pool = '1, 2, 1, 1, 1, 1'\n",
    "      FLAGS.ds_padding = \"'causal', 'causal', 'causal', 'causal', 'causal', 'causal'\"\n",
    "    FLAGS.clip_duration_ms = 1000  # standard audio file in this data set has 1 sec length\n",
    "    FLAGS.batch_size = 100\n",
    "    flags = model_flags.update_flags(FLAGS)\n",
    "    return flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = kws_set_flag()\n",
    "flags.batch_size=1\n",
    "\n",
    "kws_model = models.MODELS[flags.model_name](flags)\n",
    "weights_name='best_weights'\n",
    "kws_model.load_weights(os.path.join(flags.train_dir,weights_name)).expect_partial()\n",
    "kws_model.compile(run_eagerly = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = target_mic_arr[0].reshape([1,4000])\n",
    "out = kws_model.predict(temp_1)\n",
    "print(np.argmax(out, axis=1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Define the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.205557Z",
     "iopub.status.busy": "2022-01-26T02:32:54.204984Z",
     "iopub.status.idle": "2022-01-26T02:32:54.207129Z",
     "shell.execute_reply": "2022-01-26T02:32:54.207486Z"
    },
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "SPLIT_EPOCH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.214509Z",
     "iopub.status.busy": "2022-01-26T02:32:54.213902Z",
     "iopub.status.idle": "2022-01-26T02:32:54.216126Z",
     "shell.execute_reply": "2022-01-26T02:32:54.215581Z"
    },
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(audio, imu, normalized_train_imu, latest, epoch, batch_size, cycle):\n",
    "    global SPLIT_EPOCH\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as t_tape, tf.GradientTape() as d_tape:\n",
    "        d_loss = tf.zeros([1])\n",
    "        t_loss = tf.zeros([1])\n",
    "        imu_loss = tf.zeros([1])\n",
    "        corr_loss = tf.zeros([1])\n",
    "        abs_audio = np.abs(audio)\n",
    "        if cycle > 4:\n",
    "            translated_audio = translator(imu, training=False)\n",
    "            \n",
    "            denoised_output = denoiser([abs_audio,normalized_train_imu,pre_weight], training=True)\n",
    "            [d_loss,imu_loss,corr_loss] = denoiser_loss(translated_audio, denoised_output, abs_audio, abs_audio, normalized_train_imu, epoch, batch_size, cycle)\n",
    "    \n",
    "            gradients_of_denoiser = d_tape.gradient(d_loss, denoiser.trainable_variables)\n",
    "            denoiser_optimizer.apply_gradients(zip(gradients_of_denoiser, denoiser.trainable_variables))\n",
    "        else:\n",
    "            if epoch < SPLIT_EPOCH:\n",
    "                [denoised_audio,_] = denoiser([abs_audio,normalized_train_imu,pre_weight], training=False)\n",
    "\n",
    "                translated_audio = translator(imu, training=True)\n",
    "                if cycle == 0:\n",
    "                    t_loss = translator_loss(translated_audio, np.abs(audio), audio, cycle)\n",
    "                else:\n",
    "                    cycle_checkpoint.restore(latest)\n",
    "                    [prev_denoiser_output,_] = cycle_checkpoint.denoiser([abs_audio,normalized_train_imu,pre_weight], training=False)\n",
    "                    prev_denoiser_output = prev_denoiser_output.numpy()\n",
    "                    t_loss = translator_loss(translated_audio, prev_denoiser_output, audio, cycle)\n",
    "                gradients_of_translator = t_tape.gradient(t_loss, translator.trainable_variables)\n",
    "                translator_optimizer.apply_gradients(zip(gradients_of_translator, translator.trainable_variables))\n",
    "            else:\n",
    "                translated_audio = translator(imu, training=False)\n",
    "\n",
    "                denoised_output = denoiser([abs_audio,normalized_train_imu,pre_weight], training=True)\n",
    "                [d_loss,imu_loss,corr_loss] = denoiser_loss(translated_audio, denoised_output, abs_audio, abs_audio, normalized_train_imu, epoch, batch_size, cycle)\n",
    "\n",
    "                gradients_of_denoiser = d_tape.gradient(d_loss, denoiser.trainable_variables)\n",
    "                denoiser_optimizer.apply_gradients(zip(gradients_of_denoiser, denoiser.trainable_variables))\n",
    "            \n",
    "        return d_loss, t_loss, imu_loss, corr_loss\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.221543Z",
     "iopub.status.busy": "2022-01-26T02:32:54.220991Z",
     "iopub.status.idle": "2022-01-26T02:32:54.222703Z",
     "shell.execute_reply": "2022-01-26T02:32:54.223031Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    total_batch=100\n",
    "    train_batch_size = 128\n",
    "    best_acc = 0\n",
    "    for cycle in range(0, 2):\n",
    "        if cycle> 0:\n",
    "            latest = tf.train.latest_checkpoint(cycle_checkpoint_prefix)\n",
    "        \n",
    "        for epoch in range(0, epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            for i in range(0, total_batch):\n",
    "                train_audio, train_imu, clean_audio, labels, _, _, _= dataset_gen(batch_size=train_batch_size, ftype=\"train\", batch_ind=(i+1)*epoch)\n",
    "                normalized_train_imu = (train_imu/np.max(train_imu.reshape([train_batch_size, 20, 21])))*np.max(np.max(np.abs(train_audio).reshape([train_batch_size, 200, 21])))\n",
    "                if cycle > 0:\n",
    "                    denoiser_loss, translator_loss, imu_loss, corr_loss = train_step(train_audio, train_imu, normalized_train_imu, latest, epoch, train_batch_size, cycle)\n",
    "                \n",
    "                else:\n",
    "                    denoiser_loss, translator_loss, imu_loss, corr_loss = train_step(train_audio, train_imu, normalized_train_imu, train_audio, epoch, train_batch_size, cycle)\n",
    "                \n",
    "\n",
    "\n",
    "            # Save the model every 5 epochs\n",
    "            if (epoch) % 5 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "            print(\"Epoch: \"+str(epoch+1)+\"\\t d_loss: \"+str(denoiser_loss.numpy())+\"\\t t_loss: \"+str(translator_loss.numpy())\n",
    "                  +\"\\t i_loss: \"+str(imu_loss.numpy()) +\"\\t corr_loss: \"+str(corr_loss))\n",
    "\n",
    "            best_acc = generate_and_save_images(denoiser, translator, epoch+1, cycle, best_acc)  \n",
    "        \n",
    "        cycle_checkpoint.save(file_prefix = cycle_checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "**Generate and save images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T02:32:54.228367Z",
     "iopub.status.busy": "2022-01-26T02:32:54.227809Z",
     "iopub.status.idle": "2022-01-26T02:32:54.229850Z",
     "shell.execute_reply": "2022-01-26T02:32:54.229431Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "isExist = os.path.exists(output_parent_folder_name)\n",
    "if not isExist:\n",
    "    os.makedirs(output_parent_folder_name)\n",
    "    \n",
    "def generate_and_save_images(d_model, t_model, epoch, cycle, best_acc):\n",
    "    plot_freq = 20\n",
    "\n",
    "  # Notice `training` is set to False.\n",
    "    global output_parent_folder_name, percentile1, percentile2\n",
    "    \n",
    "    denoised_correct_classification = 0\n",
    "    \n",
    "    normalized_test_imu = (input_imu/np.max(input_imu.reshape([test_batch_size, 20, 21])))*np.max(np.max(np.abs(noisy_audio).reshape([test_batch_size, 200, 21])))\n",
    "    \n",
    "    [predictions_d,_] = d_model([np.abs(noisy_audio),normalized_test_imu,post_weight], training=False)\n",
    "    \n",
    "    predictions_t = t_model(input_imu, training=False)\n",
    "    if cycle > 0:\n",
    "        latest = tf.train.latest_checkpoint(cycle_checkpoint_prefix)\n",
    "        cycle_checkpoint.restore(latest)\n",
    "        [denoiser_output,_] = cycle_checkpoint.denoiser([np.abs(noisy_audio),normalized_test_imu,post_weight], training=False)\n",
    "        denoiser_static = denoiser_output.numpy()\n",
    "    else:\n",
    "        denoised_static = np.abs(noisy_audio)\n",
    "    \n",
    "    \n",
    "    for i in range(0, test_batch_size):\n",
    "        if epoch%plot_freq ==0 and i%10==0:\n",
    "            folder_name = os.path.join(output_parent_folder_name, \"sample_\"+str(i))\n",
    "            isExist = os.path.exists(folder_name)\n",
    "            if not isExist:\n",
    "                os.makedirs(folder_name)\n",
    "                \n",
    "            fig, ax = plt.subplots(nrows=5, ncols=1, sharex=True, dpi=80, figsize=(7, 10))\n",
    "            temp = np.asarray(clean_audio[i]).reshape([200,21])\n",
    "            temp_z = np.zeros([1,21])\n",
    "            temp = np.hstack((temp_z.T, temp.T)).T\n",
    "            img1 = librosa.display.specshow(librosa.amplitude_to_db(temp,ref=80),sr=4000, x_axis='time', y_axis='hz', ax=ax[0])\n",
    "            ax[0].set(title='Non-Interfered Audio')\n",
    "\n",
    "            temp = np.asarray(np.abs(noisy_audio[i])).reshape([200,21])\n",
    "            temp_z = np.zeros([1,21])\n",
    "            temp = np.hstack((temp_z.T, temp.T)).T\n",
    "            img2 = librosa.display.specshow(librosa.amplitude_to_db(temp,ref=80),sr=4000, x_axis='time', y_axis='hz', ax=ax[1])\n",
    "            ax[1].set(title='Interfered Audio')\n",
    "\n",
    "            temp = np.asarray(predictions_d[i]).reshape([200,21])\n",
    "            temp_z = np.zeros([1,21])\n",
    "            temp = np.hstack((temp_z.T, temp.T)).T\n",
    "            t=librosa.amplitude_to_db(temp,ref=80)\n",
    "            img3 = librosa.display.specshow(t,sr=4000, x_axis='time', y_axis='hz', ax=ax[2])\n",
    "            ax[2].set(title='Denoised Audio')\n",
    "\n",
    "            temp = np.asarray(predictions_t[1][i]).reshape([200,21])\n",
    "            temp_z = np.zeros([1,21])\n",
    "            temp = np.hstack((temp_z.T, temp.T)).T\n",
    "            t=librosa.amplitude_to_db(temp,ref=80)\n",
    "            img4 = librosa.display.specshow(t,sr=4000, x_axis='time', y_axis='hz', ax=ax[3])\n",
    "            ax[3].set(title='Translated Audio')\n",
    "\n",
    "\n",
    "        percentile_temp = percentile2 + cycle\n",
    "        mask = generate_mask(predictions_t[1][i].numpy(), np.abs(noisy_audio)[i], percentile1, percentile_temp)\n",
    "        m_max = max(mask.flatten())\n",
    "        p_max = max(predictions_d[i].numpy().flatten())\n",
    "        temp = mask.reshape([200,21])\n",
    "\n",
    "        if epoch%plot_freq == 0 and i%10==0:\n",
    "            t=librosa.amplitude_to_db(temp,ref=80)\n",
    "            img5 = librosa.display.specshow(t,sr=4000, x_axis='time', y_axis='hz', ax=ax[4])\n",
    "            ax[4].set(title='Masked Audio as Denoiser Ground Truth')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(folder_name, 'cycle_{:04d}_epoch_{:04d}_sample_{:04d}.png'.format(cycle, epoch, i)))\n",
    "    \n",
    "        reconstructed_denoised_mic = stft2time(predictions_d[i].numpy(), noisy_audio[i])\n",
    "        \n",
    "        predictions = kws_model.predict(reconstructed_denoised_mic.reshape([1,4000]))\n",
    "        predicted_labels = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "        if predicted_labels == label_dict[labels[i][0]]:\n",
    "            denoised_correct_classification += 1\n",
    "        \n",
    "    p_denoised_correct = round(denoised_correct_classification/test_batch_size, 2)\n",
    "    \n",
    "    if p_denoised_correct >= prev_acc: # validation\n",
    "        prev_acc = p_denoised_correct\n",
    "    return prev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfM4YcPVPkNO"
   },
   "source": [
    "# KWS35 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_35 ={'follow': 35,\n",
    " 'learn': 4,\n",
    " 'backward': 5,\n",
    " 'visual': 2,\n",
    " 'dog': 6,\n",
    " 'cat': 34,\n",
    " 'house': 27,\n",
    " 'bird': 21,\n",
    " 'bed': 13,\n",
    " 'tree': 17,\n",
    " 'eight': 23,\n",
    " 'marvin': 28,\n",
    " 'five': 30,\n",
    " 'go': 11,\n",
    " 'no': 24,\n",
    " 'forward': 26,\n",
    " 'down': 33,\n",
    " 'four': 20,\n",
    " 'seven': 18,\n",
    " 'wow': 3,\n",
    " 'on': 19,\n",
    " 'zero': 16,\n",
    " 'up': 12,\n",
    " 'three': 32,\n",
    " 'six': 25,\n",
    " 'left': 8,\n",
    " 'happy': 9,\n",
    " 'sheila': 29,\n",
    " 'right': 22,\n",
    " 'nine': 10,\n",
    " 'one': 15,\n",
    " 'off': 31,\n",
    " 'two': 7,\n",
    " 'yes': 36,\n",
    " 'stop': 14,\n",
    " '_silence_': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kws_set_flag_35():\n",
    "    \n",
    "    current_dir = os.getcwd()\n",
    "    MODEL_NAME = 'ds_tc_resnet'\n",
    "    # MODEL_NAME = 'svdf'\n",
    "    MODELS_PATH = os.path.join(current_dir, \"models\")\n",
    "    MODEL_PATH = os.path.join(MODELS_PATH, MODEL_NAME + \"_40k_fs_4k_35L/\")\n",
    "    MODEL_PATH\n",
    "    FLAGS = model_params.HOTWORD_MODEL_PARAMS[MODEL_NAME]\n",
    "\n",
    "    # set speech feature extractor properties\n",
    "\n",
    "    FLAGS.window_size_ms = 30.0\n",
    "    FLAGS.window_stride_ms = 10.0\n",
    "    FLAGS.mel_num_bins = 80\n",
    "    FLAGS.dct_num_features = 40\n",
    "    FLAGS.feature_type = 'mfcc_tf'\n",
    "    FLAGS.preprocess = 'raw'\n",
    "\n",
    "    # for numerical correctness of streaming and non streaming models set it to 1\n",
    "    # but for real use case streaming set it to 0\n",
    "    FLAGS.causal_data_frame_padding = 0\n",
    "\n",
    "    FLAGS.use_tf_fft = True\n",
    "    FLAGS.mel_non_zero_only = not FLAGS.use_tf_fft\n",
    "\n",
    "    # set training settings\n",
    "    FLAGS.train = 1\n",
    "    # reduced number of training steps for test only\n",
    "    # so model accuracy will be low,\n",
    "    # to improve accuracy set how_many_training_steps = '40000,40000,20000,20000'\n",
    "    FLAGS.how_many_training_steps = '40000,40000,20000,20000'\n",
    "    FLAGS.learning_rate = '0.001,0.0005,0.0001,0.00002'\n",
    "    FLAGS.lr_schedule = 'linear'\n",
    "\n",
    "    # data augmentation parameters\n",
    "    FLAGS.resample = 0.15\n",
    "    FLAGS.time_shift_ms = 100\n",
    "    FLAGS.use_spec_augment = 1\n",
    "    FLAGS.time_masks_number = 2\n",
    "    FLAGS.time_mask_max_size = 25\n",
    "    FLAGS.frequency_masks_number = 2\n",
    "    FLAGS.frequency_mask_max_size = 7\n",
    "    FLAGS.pick_deterministically = 1\n",
    "    \n",
    "    FLAGS.train_dir = MODEL_PATH\n",
    "    FLAGS.sample_rate = 4000\n",
    "    FLAGS.mel_upper_edge_hertz = 2000\n",
    "    FLAGS.model_name = MODEL_NAME\n",
    "    if MODEL_NAME == 'ds_tc_resnet':\n",
    "      # it is an example of model streaming with strided convolution, strided pooling and dilated convolution\n",
    "      FLAGS.activation = 'relu'\n",
    "      FLAGS.dropout = 0.0\n",
    "      FLAGS.ds_filters = '128, 64, 64, 64, 128, 128'\n",
    "      FLAGS.ds_filter_separable = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_repeat = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_residual = '0, 1, 1, 1, 0, 0' # residual can not be applied with stride\n",
    "    #   FLAGS.ds_kernel_size = '11, 5, 15, 7, 29, 1'\n",
    "      FLAGS.ds_kernel_size = '11, 5, 15, 17, 15, 1'\n",
    "      FLAGS.ds_dilation = '1, 1, 1, 1, 2, 1'\n",
    "      FLAGS.ds_stride = '1, 1, 1, 1, 1, 1'\n",
    "      FLAGS.ds_pool = '1, 2, 1, 1, 1, 1'\n",
    "      FLAGS.ds_padding = \"'causal', 'causal', 'causal', 'causal', 'causal', 'causal'\"\n",
    "    FLAGS.clip_duration_ms = 1000  # standard audio file in this data set has 1 sec length\n",
    "    FLAGS.batch_size = 100\n",
    "    FLAGS.wanted_words = 'visual,wow,learn,backward,dog,two,left,happy,nine,go,up,bed,stop,one,zero,tree,seven,on,four,bird,right,eight,no,six,forward,house,marvin,sheila,five,off,three,down,cat,follow,yes'\n",
    "    flags = model_flags.update_flags(FLAGS)\n",
    "    return flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_35 = kws_set_flag_35()\n",
    "flags_35.batch_size=1\n",
    "\n",
    "kws_model = models.MODELS[flags_35.model_name](flags_35)\n",
    "weights_name='best_weights'\n",
    "kws_model.load_weights(os.path.join(flags_35.train_dir,weights_name)).expect_partial()\n",
    "kws_model.compile(run_eagerly = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint.restore(latest)\n",
    "\n",
    "normalized_test_imu = (input_imu/np.max(input_imu.reshape([test_batch_size, 20, 21])))*np.max(np.max(np.abs(noisy_audio).reshape([test_batch_size, 200, 21])))\n",
    "[denoised_static,_] = checkpoint.denoiser([np.abs(noisy_audio), normalized_test_imu, post_weight], training=False).numpy()\n",
    "\n",
    "denoised_correct_classification = 0\n",
    "for i in range(0, test_batch_size):\n",
    "    reconstructed_denoised_mic = stft2time(denoised_static[i], noisy_audio[i])\n",
    "    predictions = kws_model.predict(reconstructed_denoised_mic.reshape([1,4000]))\n",
    "    predicted_labels = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    if predicted_labels == label_dict_35[labels[i][0]]:\n",
    "        denoised_correct_classification += 1\n",
    "print(\"KWS35 Accuracy: \", str(round(denoised_correct_classification/test_batch_size, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
